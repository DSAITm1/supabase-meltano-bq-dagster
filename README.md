# Supabase-BigQuery Complete Data Pipeline with dbt Analytics

A comprehensive end-to-end data pipeline that transfers data from Supabase PostgreSQL to Google BigQuery with full dbt transformations and analytics. Built with **Meltano ELT framework**, **dbt Core**, and **Dagster orchestration** for production-ready deployment.

## ğŸ¯ Complete Data Flow

```
Supabase PostgreSQL â†’ BigQuery Raw â†’ BigQuery Staging â†’ BigQuery Warehouse â†’ BigQuery Analytics
         â†“                  â†“             â†“                 â†“                    â†“
   ğŸ“‚ Source Data      ğŸ—„ï¸ Raw Layer   ğŸ”§ dbt Staging    ğŸ“Š dbt Warehouse    ï¿½ dbt Analytics
         â”‚                  â”‚             â”‚                 â”‚                    â”‚
         â””â”€â”€ Meltano ELT â”€â”€â”€â”€â”´â”€â”€â”€ dbt Transform â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                         Dagster Orchestration
```

## âœ¨ Key Features

- ğŸš€ **Multi-stage BigQuery Architecture**: Raw â†’ Staging â†’ Warehouse â†’ Analytics
- ğŸ”„ **Full dbt Transformations**: Data quality, staging models, dimension tables, fact tables
- ğŸ“Š **Analytics Ready**: Pre-built OBT (One Big Table) models for business intelligence
- ğŸ¼ **Dagster Orchestration**: Complete pipeline automation with monitoring
- ğŸ” **Data Quality**: Built-in data validation and quality checks
- ğŸ“§ **Smart Notifications**: Email alerts on pipeline completion/failure
- ğŸ·ï¸ **Clean Table Names**: Simplified naming (e.g., `geolocation` vs `supabase_olist_geolocation_dataset`)

## ğŸ“‹ Requirements

### Python Version
- **Python 3.11** (Required for Meltano compatibility)
- **Not compatible with Python 3.13+** (dependency conflicts)

### Recommended Setup
- **Production**: Meltano ELT + dbt Core + Dagster orchestration (default)
- **Development**: Direct dbt development with BigQuery
- **Orchestration**: Dagster for complete workflow management
- **Analytics**: Pre-built dbt models for immediate insights

## ğŸ—‚ï¸ Current Project Structure

```
supabase-meltano-bq-dagster/
â”œâ”€â”€ main.py                       # ğŸ¯ Complete pipeline orchestrator
â”œâ”€â”€ .env                          # ï¿½ Environment configuration
â”œâ”€â”€ requirements-bec.yaml         # ï¿½ Conda environment specification
â”œâ”€â”€ bec-dagster/                  # ğŸ¼ Dagster orchestration
â”‚   â”œâ”€â”€ dagster_pipeline.py       # ğŸ¯ Complete 26-function pipeline
â”‚   â”œâ”€â”€ start_dagster.sh          # ğŸŒ Dagster web UI launcher
â”‚   â””â”€â”€ workspace.yaml            # âš™ï¸ Dagster configuration
â”œâ”€â”€ bec-meltano/                  # ğŸ“ Production Meltano ELT pipeline
â”‚   â”œâ”€â”€ meltano.yml               # âš™ï¸ Meltano tap-postgres to target-bigquery
â”‚   â”œâ”€â”€ plugins/                  # ğŸ”Œ Meltano extractors & loaders
â”‚   â””â”€â”€ .meltano/                 # ï¿½ï¸ Meltano state & metadata
â”œâ”€â”€ bec_dbt/                      # ğŸ”§ dbt Core transformations
â”‚   â”œâ”€â”€ dbt_project.yml           # âš™ï¸ dbt project configuration
â”‚   â”œâ”€â”€ profiles.yml              # ğŸ¯ Multi-target setup (dev/warehouse/analytics)
â”‚   â”œâ”€â”€ models/                   # ğŸ“Š dbt transformation models
â”‚   â”‚   â”œâ”€â”€ staging/              # ï¿½ Data cleaning & standardization
â”‚   â”‚   â”‚   â”œâ”€â”€ sources.yml       # ğŸ“‹ Raw data sources definition
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql    # ğŸ›’ Orders staging model
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql # ğŸ‘¥ Customers staging model
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_products.sql  # ğŸ“¦ Products staging model
â”‚   â”‚   â”‚   â””â”€â”€ ... (8 more staging models)
â”‚   â”‚   â”œâ”€â”€ warehouse/            # ğŸ“‹ Dimension tables
â”‚   â”‚   â”‚       â”œâ”€â”€ dim_customers.sql    # ğŸ‘¥ Customer dimension
â”‚   â”‚   â”‚       â”œâ”€â”€ dim_products.sql     # ğŸ“¦ Product dimension
â”‚   â”‚   â”‚       â”œâ”€â”€ dim_orders.sql       # ğŸ›’ Order dimension
â”‚   â”‚   â”‚       â”œâ”€â”€ dim_dates.sql        # ğŸ“… Date dimension
â”‚   â”‚   â”‚       â””â”€â”€ ... (4 more dimensions)
â”‚   â”‚   â””â”€â”€ analytic/            # ğŸ“ˆ Analytics OBT models
â”‚   â”‚       â”œâ”€â”€ revenue_analytics_obt.sql     # ğŸ’° Revenue analytics
â”‚   â”‚       â”œâ”€â”€ customer_analytics_obt.sql    # ğŸ‘¥ Customer insights
â”‚   â”‚       â”œâ”€â”€ geographic_analytics_obt.sql  # ğŸŒ Geographic analysis
â”‚   â”‚       â””â”€â”€ ... (5 more analytics models)
â”‚   â”œâ”€â”€ macros/                   # ï¿½ dbt utility macros
â”‚   â”œâ”€â”€ tests/                    # âœ… Data quality tests
â”‚   â””â”€â”€ target/                   # ï¿½ dbt compilation artifacts
â”œâ”€â”€ requirements-bec.yaml         # ğŸ Conda environment specification (ONLY requirements file)
â””â”€â”€ README.md                     # ğŸ“– This file
```

## ğŸ—ï¸ Pipeline Architecture

### Data Layer Structure
```
ğŸ“Š BigQuery Datasets:
â”œâ”€â”€ olist_data_raw              # ğŸ—„ï¸ Raw Supabase data (Meltano target)
â”œâ”€â”€ olist_data_staging          # ğŸ”§ dbt staging models (cleaned data)
â”œâ”€â”€ olist_data_warehouse        # ğŸ“‹ dbt mart models (dimensions + facts)
â””â”€â”€ olist_data_analytics        # ğŸ“ˆ dbt analytic models (OBT tables)
```

### Pipeline Phases (26 Functions)
```
Phase 1: Raw Data Ingestion
â”œâ”€â”€ _1_staging_to_bigquery      # ğŸš€ Supabase â†’ BigQuery Raw (via Meltano)

Phase 2: dbt Staging (9 functions)
â”œâ”€â”€ _2a_processing_stg_orders                           # ğŸ›’ Orders staging
â”œâ”€â”€ _2b_processing_stg_order_items                      # ğŸ“¦ Order items staging  
â”œâ”€â”€ _2c_processing_stg_products                         # ğŸ·ï¸ Products staging
â”œâ”€â”€ _2d_processing_stg_order_reviews                    # â­ Reviews staging
â”œâ”€â”€ _2e_processing_stg_order_payments                   # ğŸ’³ Payments staging
â”œâ”€â”€ _2f_processing_stg_sellers                          # ğŸª Sellers staging
â”œâ”€â”€ _2g_processing_stg_customers                        # ğŸ‘¥ Customers staging
â”œâ”€â”€ _2h_processing_stg_geolocation                      # ğŸŒ Geography staging
â””â”€â”€ _2i_processing_stg_product_category_name_translation # ğŸŒ Translations staging

Phase 3: dbt Warehouse (9 functions)
â”œâ”€â”€ _3a_processing_dim_orders           # ğŸ›’ Orders dimension
â”œâ”€â”€ _3b_processing_dim_product          # ğŸ“¦ Products dimension
â”œâ”€â”€ _3c_processing_dim_order_reviews    # â­ Reviews dimension
â”œâ”€â”€ _3d_processing_dim_payment          # ğŸ’³ Payments dimension
â”œâ”€â”€ _3e_processing_dim_seller           # ğŸª Sellers dimension
â”œâ”€â”€ _3f_processing_dim_customer         # ğŸ‘¥ Customers dimension
â”œâ”€â”€ _3g_processing_dim_geolocation      # ğŸŒ Geography dimension
â”œâ”€â”€ _3h_processing_dim_date             # ğŸ“… Date dimension
â””â”€â”€ _3i_processing_fact_order_items     # ğŸ“Š Order items fact table

Phase 4: dbt Analytics (7 functions)
â”œâ”€â”€ _4a_processing_revenue_analytics_obt      # ğŸ’° Revenue OBT
â”œâ”€â”€ _4b_processing_orders_analytics_obt       # ğŸ›’ Orders OBT
â”œâ”€â”€ _4c_processing_delivery_analytics_obt     # ğŸšš Delivery OBT
â”œâ”€â”€ _4d_processing_customer_analytics_obt     # ğŸ‘¥ Customer OBT
â”œâ”€â”€ _4e_processing_geographic_analytics_obt   # ğŸŒ Geographic OBT
â”œâ”€â”€ _4f_processing_payment_analytics_obt      # ğŸ’³ Payment OBT
â””â”€â”€ _4g_processing_seller_analytics_obt       # ğŸª Seller OBT

Phase 5: Pipeline Monitoring
â””â”€â”€ _5_dbt_summaries                    # ğŸ“Š Complete pipeline monitoring + email alerts
```

## ğŸš€ Quick Start - Complete Pipeline

### 1. Environment Setup

**Conda Environment (Recommended & Only Option)**
```bash
# Create conda environment with Python 3.11 and all dependencies
conda env create -f requirements-bec.yaml
conda activate bec

# Verify installation
python --version  # Should show 3.11.x
meltano --version # Should show 3.7.8+
dbt --version     # Should show 1.8.8+
```


### 2. ENV file

### Environment Variables (.env)
```bash
# BigQuery Configuration
BQ_PROJECT_ID=your-gcp-project-id
BQ_LOCATION=your-bigquery-location

# BigQuery Dataset Names
TARGET_RAW_DATASET=your_raw_dataset
TARGET_STAGING_DATASET=your_staging_dataset
TARGET_BIGQUERY_DATASET=your_warehouse_dataset
TARGET_ANALYTICAL_DATASET=your_analytics_dataset

# dbt Configuration
DBT_PROFILES_DIR=./bec_dbt

# Google Cloud Authentication - Service Account JSON
GOOGLE_APPLICATION_CREDENTIALS_PATH=service-account-key.json
GOOGLE_APPLICATION_CREDENTIALS_JSON='{
  "type": "service_account",
  "project_id": "your-project-id",
  "private_key_id": "your-private-key-id",
  "private_key": "-----BEGIN PRIVATE KEY-----\nyour-private-key\n-----END PRIVATE KEY-----\n",
  "client_email": "your-service-account@your-project.iam.gserviceaccount.com",
  "client_id": "your-client-id",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/your-service-account",
  "universe_domain": "googleapis.com"
}'

# Supabase Database Configuration (for pandas.to_sql method)
DB_HOST=db.your-project.supabase.co
DB_PORT=5432
DB_NAME=postgres
DB_USER=postgres.your-project
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-supabase-anon-key
TAP_POSTGRES_PASSWORD=your-postgres-password

# Supabase connection method selector (for Dagster pipeline)
# Options: aws-1-region.pooler.supabase.com (pooler) or db.your-project.supabase.co (direct)
SUPABASE_HOST=aws-1-region.pooler.supabase.com

# Email notification configuration for Dagster pipeline summaries
# Required for _5_dbt_summaries function to send notifications

# SMTP Configuration
SMTP_SERVER=smtp.sendgrid.net
SMTP_PORT=587

# Email credentials (use app password for Gmail)
SENDER_EMAIL=your-email@gmail.com
SENDGRID_API_KEY=your-sendgrid-api-key
RECIPIENT_EMAILS=email1@company.com,email2@company.com,email3@company.com
```

### 3. Run Complete Pipeline

**Option A: Dagster Orchestration (Recommended)**
```bash
# Start Dagster web UI
cd bec-dagster/
./start_dagster.sh

# Access web interface at http://127.0.0.1:3000
# Click "Materialize All" to run the complete 26-function pipeline
```

**Option B: Direct Python Execution**
```bash
# Run the complete pipeline directly
cd bec-dagster/
python dagster_pipeline.py

# Or run specific phases
python -c "from dagster_pipeline import _1_staging_to_bigquery; print(_1_staging_to_bigquery())"
```

## ğŸ¼ Orchestration & Execution Options

### Dagster (Recommended for Production & Monitoring)
```bash
# Start Dagster web UI
cd bec-dagster/
./start_dagster.sh

# Access web interface at http://127.0.0.1:3000
# Features:
# - Complete pipeline visualization
# - Real-time execution monitoring  
# - Asset dependency graphs
# - Failed step debugging
# - Email notifications on completion
```

### dbt Development (For model development)
```bash
# Test individual dbt models
cd bec_dbt/
dbt run --select staging              # Run all staging models
dbt run --select marts.dimensions     # Run dimension models
dbt run --select analytic             # Run analytics models

# Test with different targets
dbt run --target warehouse            # Deploy to warehouse dataset
dbt run --target analytics            # Deploy to analytics dataset
```

### Meltano ELT (Production data extraction)
```bash
# Production approach with Meltano
cd bec-meltano/
meltano run tap-postgres target-bigquery

# Direct Meltano execution (used by Dagster _1 function)
```

### Individual Function Testing
```bash
# Test specific pipeline functions
cd bec-dagster/
python -c "
from dagster_pipeline import _2a_processing_stg_orders
result = _2a_processing_stg_orders()
print(f'Status: {result[\"status\"]}')
"
```

## ğŸ³ Docker Deployment

The pipeline is fully containerized with multi-service Docker deployment for production environments.

### ğŸ—ï¸ Container Architecture

- **Main Pipeline**: Comprehensive Dagster orchestration with 26-function workflow
- **Meltano ELT**: Supabase PostgreSQL to BigQuery data extraction
- **dbt Transformations**: Multi-layer BigQuery transformations (staging â†’ warehouse â†’ analytics)

### ğŸ“‹ Prerequisites

1. **Docker & Docker Compose**: Install latest versions
2. **Google Cloud Credentials**: Service account JSON files
3. **Environment Configuration**: Configure `.env` file
4. **Network Access**: Supabase and BigQuery connectivity

### ğŸš€ Quick Start

```bash
# 1. Clone and navigate to project
cd supabase-meltano-bq-dagster/

# 2. Configure environment
cp .env.example .env
# Edit .env with your actual credentials

# 3. Ensure credential files are in place
ls bec_dbt/service-account-key.json
ls bec-meltano/bigquery-credentials.json

# 4. Build and start all services
docker-compose up --build

# 5. Access Dagster web UI
open http://localhost:3000
```

### ğŸ› ï¸ Service-Specific Deployment

#### Complete Pipeline (Recommended)
```bash
# Build and run all services with logs
docker-compose up --build --force-recreate

# Run in background
docker-compose up -d --build

# View logs
docker-compose logs -f pipeline
```

#### Individual Services
```bash
# Meltano ELT only
docker-compose up meltano --build

# dbt transformations only  
docker-compose up dbt --build

# Main pipeline only
docker-compose up pipeline --build
```

### ğŸ”§ Production Deployment

#### 1. Environment Configuration
```bash
# Create production environment file
cp .env.example .env.production

# Edit with production values
vim .env.production
```

#### 2. Credential Management
```bash
# Ensure service account files are secure
chmod 600 bec_dbt/service-account-key.json
chmod 600 bec-meltano/bigquery-credentials.json

# Verify JSON validity
python -m json.tool bec_dbt/service-account-key.json
```

#### 3. Production Deployment
```bash
# Use production environment
docker-compose --env-file .env.production up -d --build

# Scale for high availability (if needed)
docker-compose up -d --scale pipeline=2

# Monitor health
docker-compose ps
docker-compose logs -f --tail=100
```

### ğŸ“Š Monitoring & Maintenance

#### Health Checks
```bash
# Check service status
docker-compose ps

# View pipeline health
curl http://localhost:3000/server_info

# Container resource usage
docker stats
```

#### Log Management
```bash
# View all logs
docker-compose logs

# Follow specific service
docker-compose logs -f meltano

# Last 100 lines with timestamps
docker-compose logs --tail=100 -t pipeline
```

#### Maintenance Operations
```bash
# Update containers
docker-compose pull
docker-compose up -d --build

# Clean up old images
docker system prune -f

# Restart specific service
docker-compose restart pipeline

# View container details
docker-compose exec pipeline python --version
```

### ğŸ” Security Best Practices

1. **Credential Security**:
   - Never commit credential files to version control
   - Use `.dockerignore` to exclude sensitive files
   - Mount credentials as read-only volumes

2. **Network Security**:
   - Use internal Docker networks for service communication
   - Expose only necessary ports (3000 for Dagster UI)
   - Consider VPN for production access

3. **Container Security**:
   - Run containers as non-root users
   - Use minimal base images (python:3.11-slim)
   - Regularly update dependencies

### ğŸ› Troubleshooting

#### Common Issues

**Container build failures**:
```bash
# Clean build cache
docker system prune -a
docker-compose build --no-cache
```

**Permission errors**:
```bash
# Fix file permissions
sudo chown -R $USER:$USER .
chmod 600 *.json
```

**Network connectivity**:
```bash
# Test network connectivity
docker-compose exec pipeline ping google.com
docker-compose exec meltano nslookup db.your-project.supabase.co
```

**BigQuery authentication**:
```bash
# Verify service account
docker-compose exec pipeline python -c "
from google.cloud import bigquery
client = bigquery.Client()
print(f'Connected to project: {client.project}')
"
```

#### Debug Mode
```bash
# Run with debug logging
MOCK_EXECUTION=true docker-compose up pipeline

# Interactive debugging
docker-compose run --rm pipeline bash
```

### ğŸ“ˆ Performance Optimization

```bash
# Adjust resource limits in docker-compose.yml
services:
  pipeline:
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
```

## â˜ï¸ Google Cloud Platform (GCP) Deployment

Deploy your production-ready Supabase-BigQuery pipeline to Google Cloud Platform with automated scaling, monitoring, and enterprise-grade security.

### ğŸ—ï¸ GCP Architecture Options

#### **Option 1: Cloud Run (Recommended for Most Cases)**
- âœ… **Serverless**: Auto-scaling from 0 to 1000+ instances
- âœ… **Cost-Effective**: Pay only for actual usage
- âœ… **Managed**: No infrastructure management
- âœ… **Fast Deployment**: Deploy in minutes

#### **Option 2: Google Kubernetes Engine (GKE)**
- âœ… **Full Control**: Complete container orchestration
- âœ… **Multi-Service**: Complex microservices architectures
- âœ… **Enterprise**: Advanced networking and security
- âœ… **High Availability**: Multi-zone deployments

#### **Option 3: Compute Engine**
- âœ… **Traditional VMs**: Full OS control
- âœ… **Custom Setup**: Specialized configurations
- âœ… **Legacy Integration**: Existing infrastructure

### ğŸš€ Quick GCP Deployment

#### Prerequisites
```bash
# 1. Install Google Cloud CLI
curl https://sdk.cloud.google.com | bash
exec -l $SHELL

# 2. Authenticate with GCP
gcloud auth login
gcloud auth configure-docker

# 3. Set your project
export GCP_PROJECT_ID="your-project-id"
export GCP_REGION="us-central1"
```

#### One-Command Cloud Run Deployment
```bash
# Deploy to Cloud Run with automated setup
GCP_PROJECT_ID=your-project ./deploy-gcp.sh production cloud-run

# Access your pipeline
echo "Dagster UI: https://supabase-bq-pipeline-xxx-uc.a.run.app"
```

### ğŸ› ï¸ Detailed GCP Setup

#### 1. Project Setup & APIs
```bash
# Create new GCP project (optional)
gcloud projects create $GCP_PROJECT_ID --name="Supabase Pipeline"
gcloud config set project $GCP_PROJECT_ID

# Enable required APIs
gcloud services enable \
    cloudbuild.googleapis.com \
    run.googleapis.com \
    container.googleapis.com \
    bigquery.googleapis.com \
    secretmanager.googleapis.com \
    scheduler.googleapis.com
```

#### 2. BigQuery Setup
```bash
# Create BigQuery datasets
bq mk --project_id=$GCP_PROJECT_ID bec_raw_olist
bq mk --project_id=$GCP_PROJECT_ID bec_staging_olist
bq mk --project_id=$GCP_PROJECT_ID bec_warehouse_olist
bq mk --project_id=$GCP_PROJECT_ID bec_analytics_olist

# Set up service account
gcloud iam service-accounts create pipeline-service \
    --display-name="Supabase Pipeline Service Account"

# Grant BigQuery permissions
gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
    --member="serviceAccount:pipeline-service@$GCP_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/bigquery.admin"
```

#### 3. Secret Management
```bash
# Store sensitive credentials in Secret Manager
echo "your-supabase-url" | gcloud secrets create supabase-url --data-file=-
echo "your-supabase-password" | gcloud secrets create supabase-password --data-file=-
echo "your-sendgrid-key" | gcloud secrets create sendgrid-api-key --data-file=-

# Upload service account key
gcloud secrets create gcp-service-account-key \
    --data-file=bec_dbt/service-account-key.json
```

#### 4. Container Registry Setup
```bash
# Build and push images to Container Registry
docker build -t gcr.io/$GCP_PROJECT_ID/supabase-bq-pipeline:latest .
docker push gcr.io/$GCP_PROJECT_ID/supabase-bq-pipeline:latest

docker build -f Dockerfile.meltano -t gcr.io/$GCP_PROJECT_ID/meltano-elt:latest .
docker push gcr.io/$GCP_PROJECT_ID/meltano-elt:latest
```

### ğŸ”„ Cloud Run Deployment

#### Deploy Main Pipeline
```bash
gcloud run deploy supabase-bq-pipeline \
    --image gcr.io/$GCP_PROJECT_ID/supabase-bq-pipeline:latest \
    --platform managed \
    --region $GCP_REGION \
    --allow-unauthenticated \
    --memory 2Gi \
    --cpu 1 \
    --timeout 3600 \
    --set-env-vars "BQ_PROJECT_ID=$GCP_PROJECT_ID" \
    --set-secrets "SUPABASE_URL=supabase-url:latest,SUPABASE_DB_PASSWORD=supabase-password:latest" \
    --port 3000
```

#### Set Up Automated Scheduling
```bash
# Daily pipeline execution at 2 AM UTC
SERVICE_URL=$(gcloud run services describe supabase-bq-pipeline --region=$GCP_REGION --format='value(status.url)')

gcloud scheduler jobs create http pipeline-daily \
    --schedule="0 2 * * *" \
    --uri="$SERVICE_URL/run-pipeline" \
    --http-method=POST \
    --time-zone="UTC"
```

### âš™ï¸ GKE Deployment (Enterprise)

#### Create GKE Cluster
```bash
# Create production-ready GKE cluster
gcloud container clusters create pipeline-cluster \
    --region=$GCP_REGION \
    --num-nodes=2 \
    --enable-autoscaling \
    --min-nodes=1 \
    --max-nodes=5 \
    --machine-type=e2-standard-2 \
    --enable-network-policy \
    --enable-ip-alias

# Get cluster credentials
gcloud container clusters get-credentials pipeline-cluster --region=$GCP_REGION
```

#### Deploy to Kubernetes
```bash
# Apply Kubernetes manifests
sed "s/YOUR_PROJECT_ID/$GCP_PROJECT_ID/g" k8s-deployment.yaml | kubectl apply -f -

# Check deployment status
kubectl get pods -n supabase-pipeline
kubectl get services -n supabase-pipeline

# Access Dagster UI
kubectl port-forward service/dagster-service 3000:80 -n supabase-pipeline
```

### ğŸ“Š Monitoring & Operations

#### Cloud Monitoring Setup
```bash
# Enable monitoring
gcloud services enable monitoring.googleapis.com

# Create alerting policies
gcloud alpha monitoring policies create \
    --policy-from-file=monitoring-policy.yaml
```

#### View Logs
```bash
# Cloud Run logs
gcloud logs read "resource.type=cloud_run_revision AND resource.labels.service_name=supabase-bq-pipeline"

# GKE logs  
kubectl logs -f deployment/dagster-pipeline -n supabase-pipeline

# BigQuery job logs
bq ls -j --max_results=10
```

#### Health Checks
```bash
# Check Cloud Run service
curl https://supabase-bq-pipeline-xxx-uc.a.run.app/health

# Check GKE service
kubectl get pods -n supabase-pipeline -w
```

### ğŸ’° Cost Optimization

#### Cloud Run Cost Management
```bash
# Set CPU allocation (only during requests)
gcloud run services update supabase-bq-pipeline \
    --cpu-allocation=request-only \
    --region=$GCP_REGION

# Configure minimum instances for warm starts
gcloud run services update supabase-bq-pipeline \
    --min-instances=0 \
    --max-instances=10 \
    --region=$GCP_REGION
```

#### BigQuery Cost Control
```bash
# Set query cost limits
bq query --use_legacy_sql=false \
    --maximum_bytes_billed=1000000 \
    "SELECT COUNT(*) FROM \`$GCP_PROJECT_ID.bec_analytics_olist.customer_analytics\`"
```

### ğŸ” Production Security

#### IAM Best Practices
```bash
# Create least-privilege service account
gcloud iam service-accounts create pipeline-minimal \
    --display-name="Pipeline Minimal Access"

# Grant only necessary permissions
gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
    --member="serviceAccount:pipeline-minimal@$GCP_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/bigquery.dataEditor"
```

#### Network Security
```bash
# Create VPC for secure communication
gcloud compute networks create pipeline-vpc --subnet-mode=custom

# Create subnet with private IP ranges
gcloud compute networks subnets create pipeline-subnet \
    --network=pipeline-vpc \
    --range=10.0.0.0/24 \
    --region=$GCP_REGION
```

### ğŸš¨ Troubleshooting GCP Deployment

#### Common Issues

**Authentication Errors**:
```bash
# Re-authenticate
gcloud auth login
gcloud auth configure-docker

# Check active account
gcloud auth list
```

**Permission Denied**:
```bash
# Check project permissions
gcloud projects get-iam-policy $GCP_PROJECT_ID

# Grant necessary roles
gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
    --member="user:your-email@domain.com" \
    --role="roles/editor"
```

**Container Build Failures**:
```bash
# Check Cloud Build logs
gcloud builds list --limit=5

# Build locally and push
docker build . -t gcr.io/$GCP_PROJECT_ID/supabase-bq-pipeline:debug
docker push gcr.io/$GCP_PROJECT_ID/supabase-bq-pipeline:debug
```

**Service Not Responding**:
```bash
# Check service logs
gcloud run services logs read supabase-bq-pipeline --region=$GCP_REGION

# Verify environment variables
gcloud run services describe supabase-bq-pipeline --region=$GCP_REGION
```

### ğŸ“‹ Production Checklist

- [ ] âœ… GCP project created with billing enabled
- [ ] âœ… Required APIs enabled (Cloud Run, BigQuery, Secret Manager)
- [ ] âœ… Service account created with minimal permissions
- [ ] âœ… BigQuery datasets created (raw, staging, warehouse, analytics)
- [ ] âœ… Secrets stored in Secret Manager
- [ ] âœ… Container images built and pushed to Container Registry
- [ ] âœ… Cloud Run service deployed with health checks
- [ ] âœ… Cloud Scheduler configured for automated runs
- [ ] âœ… Monitoring and alerting set up
- [ ] âœ… Backup and disaster recovery planned
- [ ] âœ… Cost monitoring and budgets configured

### ğŸ¯ Next Steps

1. **Deploy to GCP**: Use `./deploy-gcp.sh production cloud-run`
2. **Configure Monitoring**: Set up alerts for pipeline failures
3. **Test Pipeline**: Run end-to-end validation
4. **Scale Setup**: Configure auto-scaling based on demand
5. **Optimize Costs**: Set up BigQuery slot commitments if needed

Your Supabase-BigQuery pipeline is now ready for enterprise-grade production deployment on Google Cloud Platform! ğŸš€

## ğŸ¤ Contributing

1. **Environment Setup**: Ensure Python 3.11 conda environment: `conda activate bec`
2. **Install Dependencies**: `conda env create -f requirements-bec.yaml`
3. **Configure Environment**: Copy `.env.example` to `.env` and configure
4. **Test Pipeline**: Run with `MOCK_EXECUTION=true` for safe testing
5. **Test Components**: Verify Meltano, dbt, and Dagster integration
6. **Update Documentation**: Update README for any new features or changes

### Development Workflow
```bash
# 1. Test individual dbt models
cd bec_dbt/
dbt run --select staging

# 2. Test full pipeline with mock data
cd bec-dagster/
MOCK_EXECUTION=true python dagster_pipeline.py

# 3. Test production pipeline
./start_dagster.sh  # Then click "Materialize All" in web UI
```

### dbt Profiles Configuration
The pipeline uses multiple dbt targets for different datasets:
```yaml
# profiles.yml
bec_dbt:
  target: dev
  outputs:
    dev:          # Development (staging dataset)
    warehouse:    # Production warehouse dataset
    analytics:    # Analytics OBT dataset
```

## ğŸ”§ Development Notes

### Python Version Compatibility
- **Python 3.11**: âœ… Fully supported (recommended)
- **Python 3.12**: âš ï¸ Limited support (some dependency issues)
- **Python 3.13+**: âŒ Not supported (major dependency conflicts)

### Production vs Development
- **Production**: Use Dagster orchestration with full 26-function pipeline
- **Development**: Use individual dbt commands for model development
- **Testing**: Use MOCK_EXECUTION=true for pipeline testing without real data
- **Monitoring**: Dagster web UI provides complete pipeline visibility

### Data Quality & Transformations
The pipeline includes comprehensive data quality checks:
- **Staging Layer**: Data cleaning, standardization, and validation
- **Warehouse Layer**: Business logic and dimensional modeling
- **Analytics Layer**: Pre-aggregated OBT models for BI tools
- **Quality Tests**: dbt tests for data integrity and business rules

### Table Naming Optimization
Recent improvements include simplified table naming:
- **Before**: `supabase_olist_geolocation_dataset`
- **After**: `geolocation`
- **Benefits**: Cleaner BigQuery experience, better readability, simplified SQL

### Pipeline Monitoring
- **Email Notifications**: Automatic alerts on pipeline completion/failure
- **Function Status Tracking**: Individual function success/failure monitoring  
- **Record Count Validation**: Source vs destination record count verification
- **Error Handling**: Comprehensive error capture and reporting

### Troubleshooting
1. **Meltano connection issues**: Check Supabase host and credentials
2. **dbt compilation errors**: Verify profiles.yml and BigQuery datasets exist
3. **Dagster asset failures**: Check individual function logs in web UI
4. **BigQuery authentication**: Verify GOOGLE_APPLICATION_CREDENTIALS_JSON format
5. **Empty staging tables**: Ensure raw data loaded successfully from Supabase
6. **Email notifications**: Check Gmail app password configuration

## ğŸ“Š Pipeline Features

- âœ… **Complete Supabase to BigQuery Integration** with optimized table naming
- âœ… **Multi-layer BigQuery Architecture** (Raw â†’ Staging â†’ Warehouse â†’ Analytics)
- âœ… **Full dbt Core Integration** with 26 automated transformation functions
- âœ… **Production-ready Meltano ELT** with tap-postgres to target-bigquery
- âœ… **Comprehensive Dagster Orchestration** with web UI monitoring
- âœ… **Data Quality & Validation** at every pipeline stage
- âœ… **Email Notifications** for pipeline success/failure alerts
- âœ… **Dimensional Modeling** with proper fact and dimension tables
- âœ… **Pre-built Analytics Models** (OBT) ready for BI tools
- âœ… **Environment-based Configuration** for dev/warehouse/analytics targets
- âœ… **Error Handling & Recovery** with detailed logging
- âœ… **State Management** and incremental processing capabilities
- âœ… **Mock Execution Mode** for testing and development
- âœ… **Record Count Validation** between source and target systems

## ğŸ—‚ï¸ Key Components

### Data Transformation Scripts
- **`bec-dagster/dagster_pipeline.py`**: Complete 26-function orchestrated pipeline
- **`bec_dbt/models/staging/`**: 9 dbt staging models for data cleaning
- **`bec_dbt/models/marts/dimensions/`**: 8 dbt dimension models  
- **`bec_dbt/models/analytic/`**: 7 dbt analytics OBT models

### Data Integration & Pipeline
- **`bec-meltano/meltano.yml`**: Production Supabase to BigQuery ELT configuration
- **`bec_dbt/profiles.yml`**: Multi-target dbt deployment configuration  
- **`bec-dagster/start_dagster.sh`**: Dagster web UI launcher for monitoring

### Configuration & Environment
- **`.env`**: Complete environment configuration for all pipeline components
- **`requirements-bec.yaml`**: Conda environment with Python 3.11, Meltano, dbt, Dagster

## ğŸ“ˆ Analytics Models Ready for BI

The pipeline produces 7 pre-built analytics tables optimized for business intelligence:

1. **`revenue_analytics_obt`** - Revenue insights and trends
2. **`customer_analytics_obt`** - Customer behavior and segmentation  
3. **`orders_analytics_obt`** - Order patterns and fulfillment metrics
4. **`delivery_analytics_obt`** - Delivery performance and logistics
5. **`geographic_analytics_obt`** - Geographic sales distribution
6. **`payment_analytics_obt`** - Payment method analysis
7. **`seller_analytics_obt`** - Seller performance metrics

All models include comprehensive business metrics, KPIs, and are optimized for visualization tools like Tableau, Power BI, or Looker.

## ğŸ¤ Contributing

1. **Environment Setup**: Ensure Python 3.11 conda environment: `conda activate bec`
2. **Install Dependencies**: `conda env create -f requirements-bec.yaml`
3. **Test Pipeline**: Run with `MOCK_EXECUTION=true` for safe testing
4. **Test Components**: Verify Meltano, dbt, and Dagster integration
5. **Update Documentation**: Update README for any new features or changes

### Development Workflow
```bash
# 1. Test individual dbt models
cd bec_dbt/
dbt run --select staging

# 2. Test full pipeline with mock data
cd bec-dagster/
MOCK_EXECUTION=true python dagster_pipeline.py

# 3. Test production pipeline
./start_dagster.sh  # Then click "Materialize All" in web UI
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ¯ Quick Summary

This is a **production-ready data pipeline** that:
- Extracts data from **Supabase PostgreSQL**
- Loads into **BigQuery** with **4-layer architecture** (Raw â†’ Staging â†’ Warehouse â†’ Analytics)  
- Transforms data using **dbt Core** with **26 automated functions**
- Orchestrates everything with **Dagster** including **email notifications**
- Provides **7 pre-built analytics models** ready for BI tools
- Features **clean table naming** and **comprehensive monitoring**

**Perfect for**: E-commerce analytics, customer insights, sales reporting, and business intelligence workflows.

**ğŸš€ Start with**: `./start_dagster.sh` â†’ Open http://127.0.0.1:3000 â†’ Click "Materialize All"
