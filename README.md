# Supabase-BigQuery Complete Data Pipeline with dbt Analytics

A comprehensive end-to-end data pipeline that transfers data from Supabase PostgreSQL to Google BigQuery with full dbt transformations and analytics. Built with **Meltano ELT framework**, **dbt Core**, and **Dagster orchestration** for production-ready deployment.

## ğŸ¯ Complete Data Flow

```
Supabase PostgreSQL â†’ BigQuery Raw â†’ BigQuery Staging â†’ BigQuery Warehouse â†’ BigQuery Analytics
         â†“                  â†“             â†“                 â†“                    â†“
   ğŸ“‚ Source Data      ğŸ—„ï¸ Raw Layer   ğŸ”§ dbt Staging    ğŸ“Š dbt Warehouse    ï¿½ dbt Analytics
         â”‚                  â”‚             â”‚                 â”‚                    â”‚
         â””â”€â”€ Meltano ELT â”€â”€â”€â”€â”´â”€â”€â”€ dbt Transform â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                         Dagster Orchestration
```

## âœ¨ Key Features

- ğŸš€ **Multi-stage BigQuery Architecture**: Raw â†’ Staging â†’ Warehouse â†’ Analytics
- ğŸ”„ **Full dbt Transformations**: Data quality, staging models, dimension tables, fact tables
- ğŸ“Š **Analytics Ready**: Pre-built OBT (One Big Table) models for business intelligence
- ğŸ¼ **Dagster Orchestration**: Complete pipeline automation with monitoring
- ğŸ” **Data Quality**: Built-in data validation and quality checks
- ğŸ“§ **Smart Notifications**: Email alerts on pipeline completion/failure
- ğŸ·ï¸ **Clean Table Names**: Simplified naming (e.g., `geolocation` vs `supabase_olist_geolocation_dataset`)

## ğŸ“‹ Requirements

### Python Version
- **Python 3.11** (Required for Meltano compatibility)
- **Not compatible with Python 3.13+** (dependency conflicts)

### Recommended Setup
- **Production**: Meltano ELT + dbt Core + Dagster orchestration (default)
- **Development**: Direct dbt development with BigQuery
- **Orchestration**: Dagster for complete workflow management
- **Analytics**: Pre-built dbt models for immediate insights

## ğŸ—‚ï¸ Current Project Structure

```
supabase-meltano-bq-dagster/
â”œâ”€â”€ main.py                       # ğŸ¯ Complete pipeline orchestrator
â”œâ”€â”€ .env                          # ï¿½ Environment configuration
â”œâ”€â”€ requirements-bec.yaml         # ï¿½ Conda environment specification
â”œâ”€â”€ bec-dagster/                  # ğŸ¼ Dagster orchestration
â”‚   â”œâ”€â”€ dagster_pipeline.py       # ğŸ¯ Complete 26-function pipeline
â”‚   â”œâ”€â”€ start_dagster.sh          # ğŸŒ Dagster web UI launcher
â”‚   â””â”€â”€ workspace.yaml            # âš™ï¸ Dagster configuration
â”œâ”€â”€ bec-meltano/                  # ğŸ“ Production Meltano ELT pipeline
â”‚   â”œâ”€â”€ meltano.yml               # âš™ï¸ Meltano tap-postgres to target-bigquery
â”‚   â”œâ”€â”€ plugins/                  # ğŸ”Œ Meltano extractors & loaders
â”‚   â””â”€â”€ .meltano/                 # ï¿½ï¸ Meltano state & metadata
â”œâ”€â”€ bec_dbt/                      # ğŸ”§ dbt Core transformations
â”‚   â”œâ”€â”€ dbt_project.yml           # âš™ï¸ dbt project configuration
â”‚   â”œâ”€â”€ profiles.yml              # ğŸ¯ Multi-target setup (dev/warehouse/analytics)
â”‚   â”œâ”€â”€ models/                   # ğŸ“Š dbt transformation models
â”‚   â”‚   â”œâ”€â”€ staging/              # ï¿½ Data cleaning & standardization
â”‚   â”‚   â”‚   â”œâ”€â”€ sources.yml       # ğŸ“‹ Raw data sources definition
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql    # ğŸ›’ Orders staging model
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql # ğŸ‘¥ Customers staging model
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_products.sql  # ğŸ“¦ Products staging model
â”‚   â”‚   â”‚   â””â”€â”€ ... (8 more staging models)
â”‚   â”‚   â”œâ”€â”€ marts/               # ğŸ“Š Business logic & dimensions
â”‚   â”‚   â”‚   â””â”€â”€ dimensions/      # ğŸ“‹ Dimension tables
â”‚   â”‚   â”‚       â”œâ”€â”€ dim_customers.sql    # ğŸ‘¥ Customer dimension
â”‚   â”‚   â”‚       â”œâ”€â”€ dim_products.sql     # ğŸ“¦ Product dimension
â”‚   â”‚   â”‚       â”œâ”€â”€ dim_orders.sql       # ğŸ›’ Order dimension
â”‚   â”‚   â”‚       â”œâ”€â”€ dim_dates.sql        # ğŸ“… Date dimension
â”‚   â”‚   â”‚       â””â”€â”€ ... (4 more dimensions)
â”‚   â”‚   â””â”€â”€ analytic/            # ğŸ“ˆ Analytics OBT models
â”‚   â”‚       â”œâ”€â”€ revenue_analytics_obt.sql     # ğŸ’° Revenue analytics
â”‚   â”‚       â”œâ”€â”€ customer_analytics_obt.sql    # ğŸ‘¥ Customer insights
â”‚   â”‚       â”œâ”€â”€ geographic_analytics_obt.sql  # ğŸŒ Geographic analysis
â”‚   â”‚       â””â”€â”€ ... (5 more analytics models)
â”‚   â”œâ”€â”€ macros/                   # ï¿½ dbt utility macros
â”‚   â”œâ”€â”€ tests/                    # âœ… Data quality tests
â”‚   â””â”€â”€ target/                   # ï¿½ dbt compilation artifacts
â”œâ”€â”€ requirements-bec.yaml         # ğŸ Conda environment specification (ONLY requirements file)
â””â”€â”€ README.md                     # ğŸ“– This file
```

## ğŸ—ï¸ Pipeline Architecture

### Data Layer Structure
```
ğŸ“Š BigQuery Datasets:
â”œâ”€â”€ olist_data_raw              # ğŸ—„ï¸ Raw Supabase data (Meltano target)
â”œâ”€â”€ olist_data_staging          # ğŸ”§ dbt staging models (cleaned data)
â”œâ”€â”€ olist_data_warehouse        # ğŸ“‹ dbt mart models (dimensions + facts)
â””â”€â”€ olist_data_analytics        # ğŸ“ˆ dbt analytic models (OBT tables)
```

### Pipeline Phases (26 Functions)
```
Phase 1: Raw Data Ingestion
â”œâ”€â”€ _1_staging_to_bigquery      # ğŸš€ Supabase â†’ BigQuery Raw (via Meltano)

Phase 2: dbt Staging (9 functions)
â”œâ”€â”€ _2a_processing_stg_orders                           # ğŸ›’ Orders staging
â”œâ”€â”€ _2b_processing_stg_order_items                      # ğŸ“¦ Order items staging  
â”œâ”€â”€ _2c_processing_stg_products                         # ğŸ·ï¸ Products staging
â”œâ”€â”€ _2d_processing_stg_order_reviews                    # â­ Reviews staging
â”œâ”€â”€ _2e_processing_stg_order_payments                   # ğŸ’³ Payments staging
â”œâ”€â”€ _2f_processing_stg_sellers                          # ğŸª Sellers staging
â”œâ”€â”€ _2g_processing_stg_customers                        # ğŸ‘¥ Customers staging
â”œâ”€â”€ _2h_processing_stg_geolocation                      # ğŸŒ Geography staging
â””â”€â”€ _2i_processing_stg_product_category_name_translation # ğŸŒ Translations staging

Phase 3: dbt Warehouse (9 functions)
â”œâ”€â”€ _3a_processing_dim_orders           # ğŸ›’ Orders dimension
â”œâ”€â”€ _3b_processing_dim_product          # ğŸ“¦ Products dimension
â”œâ”€â”€ _3c_processing_dim_order_reviews    # â­ Reviews dimension
â”œâ”€â”€ _3d_processing_dim_payment          # ğŸ’³ Payments dimension
â”œâ”€â”€ _3e_processing_dim_seller           # ğŸª Sellers dimension
â”œâ”€â”€ _3f_processing_dim_customer         # ğŸ‘¥ Customers dimension
â”œâ”€â”€ _3g_processing_dim_geolocation      # ğŸŒ Geography dimension
â”œâ”€â”€ _3h_processing_dim_date             # ğŸ“… Date dimension
â””â”€â”€ _3i_processing_fact_order_items     # ğŸ“Š Order items fact table

Phase 4: dbt Analytics (7 functions)
â”œâ”€â”€ _4a_processing_revenue_analytics_obt      # ğŸ’° Revenue OBT
â”œâ”€â”€ _4b_processing_orders_analytics_obt       # ğŸ›’ Orders OBT
â”œâ”€â”€ _4c_processing_delivery_analytics_obt     # ğŸšš Delivery OBT
â”œâ”€â”€ _4d_processing_customer_analytics_obt     # ğŸ‘¥ Customer OBT
â”œâ”€â”€ _4e_processing_geographic_analytics_obt   # ğŸŒ Geographic OBT
â”œâ”€â”€ _4f_processing_payment_analytics_obt      # ğŸ’³ Payment OBT
â””â”€â”€ _4g_processing_seller_analytics_obt       # ğŸª Seller OBT

Phase 5: Pipeline Monitoring
â””â”€â”€ _5_dbt_summaries                    # ğŸ“Š Complete pipeline monitoring + email alerts
```

## ğŸš€ Quick Start - Complete Pipeline

### 1. Environment Setup

**Conda Environment (Recommended & Only Option)**
```bash
# Create conda environment with Python 3.11 and all dependencies
conda env create -f requirements-bec.yaml
conda activate bec

# Verify installation
python --version  # Should show 3.11.x
meltano --version # Should show 3.7.8+
dbt --version     # Should show 1.8.8+
```

### 2. Configure Environment Variables
```bash
# Copy template and edit with your credentials
cp .env.example .env
nano .env  # Add your Supabase, BigQuery, and email credentials
```

**Required Environment Variables:**
```bash
# Supabase Configuration
SUPABASE_HOST=your-supabase-host
SUPABASE_USERNAME=your-username
SUPABASE_PASSWORD=your-password
SUPABASE_DATABASE=your-database

# BigQuery Configuration
BQ_PROJECT_ID=your-gcp-project-id
TARGET_RAW_DATASET=olist_data_raw
TARGET_STAGING_DATASET=olist_data_staging
TARGET_WAREHOUSE_DATASET=olist_data_warehouse
TARGET_ANALYTICAL_DATASET=olist_data_analytics
GOOGLE_APPLICATION_CREDENTIALS_JSON='{"type": "service_account", ...}'

# Email Notifications (optional)
EMAIL_FROM=your-email@gmail.com
EMAIL_PASSWORD=your-app-password
EMAIL_TO=notifications@company.com
```

### 3. Run Complete Pipeline

**Option A: Dagster Orchestration (Recommended)**
```bash
# Start Dagster web UI
cd bec-dagster/
./start_dagster.sh

# Access web interface at http://127.0.0.1:3000
# Click "Materialize All" to run the complete 26-function pipeline
```

**Option B: Direct Python Execution**
```bash
# Run the complete pipeline directly
cd bec-dagster/
python dagster_pipeline.py

# Or run specific phases
python -c "from dagster_pipeline import _1_staging_to_bigquery; print(_1_staging_to_bigquery())"
```

## ğŸ¼ Orchestration & Execution Options

### Dagster (Recommended for Production & Monitoring)
```bash
# Start Dagster web UI
cd bec-dagster/
./start_dagster.sh

# Access web interface at http://127.0.0.1:3000
# Features:
# - Complete pipeline visualization
# - Real-time execution monitoring  
# - Asset dependency graphs
# - Failed step debugging
# - Email notifications on completion
```

### dbt Development (For model development)
```bash
# Test individual dbt models
cd bec_dbt/
dbt run --select staging              # Run all staging models
dbt run --select marts.dimensions     # Run dimension models
dbt run --select analytic             # Run analytics models

# Test with different targets
dbt run --target warehouse            # Deploy to warehouse dataset
dbt run --target analytics            # Deploy to analytics dataset
```

### Meltano ELT (Production data extraction)
```bash
# Production approach with Meltano
cd bec-meltano/
meltano run tap-postgres target-bigquery

# Direct Meltano execution (used by Dagster _1 function)
```

### Individual Function Testing
```bash
# Test specific pipeline functions
cd bec-dagster/
python -c "
from dagster_pipeline import _2a_processing_stg_orders
result = _2a_processing_stg_orders()
print(f'Status: {result[\"status\"]}')
"
```

## ğŸ³ Docker Deployment

The pipeline is optimized for Docker deployment using Meltano and dbt:

```bash
cd bec-meltano/
# Production deployment with Meltano containerization
meltano run tap-postgres target-bigquery

# Or use dbt Docker deployment for transformations
cd bec_dbt/
dbt run --target warehouse
```

## ğŸ¤ Contributing

1. **Environment Setup**: Ensure Python 3.11 conda environment: `conda activate bec`
2. **Install Dependencies**: `conda env create -f requirements-bec.yaml`
3. **Configure Environment**: Copy `.env.example` to `.env` and configure
4. **Test Pipeline**: Run with `MOCK_EXECUTION=true` for safe testing
5. **Test Components**: Verify Meltano, dbt, and Dagster integration
6. **Update Documentation**: Update README for any new features or changes

### Development Workflow
```bash
# 1. Test individual dbt models
cd bec_dbt/
dbt run --select staging

# 2. Test full pipeline with mock data
cd bec-dagster/
MOCK_EXECUTION=true python dagster_pipeline.py

# 3. Test production pipeline
./start_dagster.sh  # Then click "Materialize All" in web UI
```

## âš™ï¸ Configuration

### Environment Variables (.env)
```bash
# Supabase PostgreSQL Configuration
SUPABASE_HOST=your-supabase-host.supabase.co
SUPABASE_USERNAME=your_username
SUPABASE_PASSWORD=your_password
SUPABASE_DATABASE=postgres

# Google BigQuery Configuration
BQ_PROJECT_ID=your-gcp-project-id
TARGET_RAW_DATASET=olist_data_raw
TARGET_STAGING_DATASET=olist_data_staging  
TARGET_WAREHOUSE_DATASET=olist_data_warehouse
TARGET_ANALYTICAL_DATASET=olist_data_analytics
GOOGLE_APPLICATION_CREDENTIALS_JSON='{"type": "service_account", ...}'

# Email Notifications (optional)
EMAIL_FROM=your-notifications@gmail.com
EMAIL_PASSWORD=your-gmail-app-password
EMAIL_TO=team@company.com

# Pipeline Configuration
MOCK_EXECUTION=false                    # Set to true for testing without real data
```

### dbt Profiles Configuration
The pipeline uses multiple dbt targets for different datasets:
```yaml
# profiles.yml
bec_dbt:
  target: dev
  outputs:
    dev:          # Development (staging dataset)
    warehouse:    # Production warehouse dataset
    analytics:    # Analytics OBT dataset
```

## ğŸ”§ Development Notes

### Python Version Compatibility
- **Python 3.11**: âœ… Fully supported (recommended)
- **Python 3.12**: âš ï¸ Limited support (some dependency issues)
- **Python 3.13+**: âŒ Not supported (major dependency conflicts)

### Production vs Development
- **Production**: Use Dagster orchestration with full 26-function pipeline
- **Development**: Use individual dbt commands for model development
- **Testing**: Use MOCK_EXECUTION=true for pipeline testing without real data
- **Monitoring**: Dagster web UI provides complete pipeline visibility

### Data Quality & Transformations
The pipeline includes comprehensive data quality checks:
- **Staging Layer**: Data cleaning, standardization, and validation
- **Warehouse Layer**: Business logic and dimensional modeling
- **Analytics Layer**: Pre-aggregated OBT models for BI tools
- **Quality Tests**: dbt tests for data integrity and business rules

### Table Naming Optimization
Recent improvements include simplified table naming:
- **Before**: `supabase_olist_geolocation_dataset`
- **After**: `geolocation`
- **Benefits**: Cleaner BigQuery experience, better readability, simplified SQL

### Pipeline Monitoring
- **Email Notifications**: Automatic alerts on pipeline completion/failure
- **Function Status Tracking**: Individual function success/failure monitoring  
- **Record Count Validation**: Source vs destination record count verification
- **Error Handling**: Comprehensive error capture and reporting

### Troubleshooting
1. **Meltano connection issues**: Check Supabase host and credentials
2. **dbt compilation errors**: Verify profiles.yml and BigQuery datasets exist
3. **Dagster asset failures**: Check individual function logs in web UI
4. **BigQuery authentication**: Verify GOOGLE_APPLICATION_CREDENTIALS_JSON format
5. **Empty staging tables**: Ensure raw data loaded successfully from Supabase
6. **Email notifications**: Check Gmail app password configuration

## ğŸ“Š Pipeline Features

- âœ… **Complete Supabase to BigQuery Integration** with optimized table naming
- âœ… **Multi-layer BigQuery Architecture** (Raw â†’ Staging â†’ Warehouse â†’ Analytics)
- âœ… **Full dbt Core Integration** with 26 automated transformation functions
- âœ… **Production-ready Meltano ELT** with tap-postgres to target-bigquery
- âœ… **Comprehensive Dagster Orchestration** with web UI monitoring
- âœ… **Data Quality & Validation** at every pipeline stage
- âœ… **Email Notifications** for pipeline success/failure alerts
- âœ… **Dimensional Modeling** with proper fact and dimension tables
- âœ… **Pre-built Analytics Models** (OBT) ready for BI tools
- âœ… **Environment-based Configuration** for dev/warehouse/analytics targets
- âœ… **Error Handling & Recovery** with detailed logging
- âœ… **State Management** and incremental processing capabilities
- âœ… **Mock Execution Mode** for testing and development
- âœ… **Record Count Validation** between source and target systems

## ğŸ—‚ï¸ Key Components

### Data Transformation Scripts
- **`bec-dagster/dagster_pipeline.py`**: Complete 26-function orchestrated pipeline
- **`bec_dbt/models/staging/`**: 9 dbt staging models for data cleaning
- **`bec_dbt/models/marts/dimensions/`**: 8 dbt dimension models  
- **`bec_dbt/models/analytic/`**: 7 dbt analytics OBT models

### Data Integration & Pipeline
- **`bec-meltano/meltano.yml`**: Production Supabase to BigQuery ELT configuration
- **`bec_dbt/profiles.yml`**: Multi-target dbt deployment configuration  
- **`bec-dagster/start_dagster.sh`**: Dagster web UI launcher for monitoring

### Configuration & Environment
- **`.env`**: Complete environment configuration for all pipeline components
- **`requirements-bec.yaml`**: Conda environment with Python 3.11, Meltano, dbt, Dagster

## ğŸ“ˆ Analytics Models Ready for BI

The pipeline produces 7 pre-built analytics tables optimized for business intelligence:

1. **`revenue_analytics_obt`** - Revenue insights and trends
2. **`customer_analytics_obt`** - Customer behavior and segmentation  
3. **`orders_analytics_obt`** - Order patterns and fulfillment metrics
4. **`delivery_analytics_obt`** - Delivery performance and logistics
5. **`geographic_analytics_obt`** - Geographic sales distribution
6. **`payment_analytics_obt`** - Payment method analysis
7. **`seller_analytics_obt`** - Seller performance metrics

All models include comprehensive business metrics, KPIs, and are optimized for visualization tools like Tableau, Power BI, or Looker.

## ğŸ¤ Contributing

1. **Environment Setup**: Ensure Python 3.11 conda environment: `conda activate bec`
2. **Install Dependencies**: `conda env create -f requirements-bec.yaml`
3. **Configure Environment**: Copy `.env.example` to `.env` and configure
4. **Test Pipeline**: Run with `MOCK_EXECUTION=true` for safe testing
5. **Test Components**: Verify Meltano, dbt, and Dagster integration
6. **Update Documentation**: Update README for any new features or changes

### Development Workflow
```bash
# 1. Test individual dbt models
cd bec_dbt/
dbt run --select staging

# 2. Test full pipeline with mock data
cd bec-dagster/
MOCK_EXECUTION=true python dagster_pipeline.py

# 3. Test production pipeline
./start_dagster.sh  # Then click "Materialize All" in web UI
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ¯ Quick Summary

This is a **production-ready data pipeline** that:
- Extracts data from **Supabase PostgreSQL**
- Loads into **BigQuery** with **4-layer architecture** (Raw â†’ Staging â†’ Warehouse â†’ Analytics)  
- Transforms data using **dbt Core** with **26 automated functions**
- Orchestrates everything with **Dagster** including **email notifications**
- Provides **7 pre-built analytics models** ready for BI tools
- Features **clean table naming** and **comprehensive monitoring**

**Perfect for**: E-commerce analytics, customer insights, sales reporting, and business intelligence workflows.

**ğŸš€ Start with**: `./start_dagster.sh` â†’ Open http://127.0.0.1:3000 â†’ Click "Materialize All"
